# -*- coding: utf-8 -*-
"""final_classification_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gZjrK7yues-F2gSk7iBFRvc7jQIp5dEt
"""

import torch
import nltk
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from nltk.tokenize import sent_tokenize

# Download and set up the sentence tokenizer
nltk.download('punkt_tab')

# Load the pre-trained BERT model and tokenizer
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

MODEL_NAME = "adeeteya/distilbert_base_uncased_finetuned_tos"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)
model.to(device)
# Define the labels
LABELS = ['Risk', 'Safe', 'Information']

CONFIDENCE_THRESHOLD = 0.7

# Preprocessing function for the sentences
def preprocess(sentence):
    """Tokenize and prepare inputs for the BERT model."""
    return tokenizer(sentence, truncation=True, padding=True, max_length=512, return_tensors='pt')

# Function to classify a single sentence
def classify_sentence(sentence):
    """Classify a sentence into Risk, Safe, or Information."""
    inputs = preprocess(sentence)
    with torch.no_grad():
        outputs = model(**inputs)
    probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)
    predicted_class = torch.argmax(probabilities).item()
    confidence = probabilities[0, predicted_class].item()

    # Skip sentences with low confidence
    if confidence < CONFIDENCE_THRESHOLD and predicted_class==0:
        return None, None
    elif confidence+0.1 < CONFIDENCE_THRESHOLD:
        return None, None
    return LABELS[predicted_class], confidence

# Function to process the entire document
def classify_document(document):
    """Split a document into sentences, classify each sentence."""
    sentences = sent_tokenize(document)
    seen_sentences = set()
    classified_sentences = {
        "Risk": [],
        "Safe": [],
        "Information": []
    }
    for sentence in sentences:
      # Skip duplicate sentences
        if sentence in seen_sentences:
            continue
        seen_sentences.add(sentence)
        classification, confidence = classify_sentence(sentence)
        if classification:
            classified_sentences[classification].append((sentence, confidence))
    return classified_sentences

def assign_grade(classified_sentences):
    """Assign a grade based on the counts of Risk and Safe sentences."""
    risk_count = len(classified_sentences['Risk'])
    safe_count = len(classified_sentences['Safe'])

    if risk_count < 3 and safe_count > 0:
        return "A"  # No risks, some safes
    elif risk_count < 10 and safe_count > 5:
        return "B"  # Very few risks compared to safes
    elif risk_count < 15 and safe_count > 2:
        return "C"  # Moderate risk
    elif risk_count < 20:
        return "D"  # Risks approaching safes
    else:
        return "F"  # More risks than safes

with open('input.txt') as f:
  input_document=f.read()

classified_output = classify_document(input_document)
print("Classified Sentences:")
grade = assign_grade(classified_output)
print(f"\nService Grade: {grade}\n")

for label, sentences in classified_output.items():
    if sentences:
      print(f"\n{label}:")
    for sentence,confidence in sentences:
        print(f"  - {sentence} (Confidence: {confidence:.2f})")